{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "its done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pdb\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "import random \n",
    "import time\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from tqdm import *\n",
    "\n",
    "from functools import wraps\n",
    "from time import time as _timenow \n",
    "from sys import stderr\n",
    "print('its done') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR-10 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/data_batch_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e29c21f21630>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtst_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpickled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrn_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtrn_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_cifar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'itsdone'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e29c21f21630>\u001b[0m in \u001b[0;36mload_cifar\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbatchName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/data_batch_{0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0munpickled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtrn_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpickled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtrn_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpickled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e29c21f21630>\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrn_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/data_batch_1'"
     ]
    }
   ],
   "source": [
    "def load_cifar():\n",
    "    \n",
    "    trn_data, trn_labels, tst_data, tst_labels = [], [], [], []\n",
    "    def unpickle(file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            data = pickle.load(fo, encoding='latin1')\n",
    "        return data\n",
    "    \n",
    "    for i in trange(1):\n",
    "        batchName = './data/data_batch_{0}'.format(i + 1)\n",
    "        unpickled = unpickle(batchName)\n",
    "        trn_data.extend(unpickled['data'])\n",
    "        trn_labels.extend(unpickled['labels'])\n",
    "    unpickled = unpickle('./data/test_batch')\n",
    "    tst_data.extend(unpickled['data'])\n",
    "    tst_labels.extend(unpickled['labels'])\n",
    "    return trn_data, trn_labels, tst_data, tst_labels\n",
    "trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "print('itsdone')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(train,test):\n",
    "    scaler = preprocessing.StandardScaler().fit(train)\n",
    "    train_data = scaler.transform(train)\n",
    "    test_data = scaler.transform(test)\n",
    "    ''' pre-processes the given image\n",
    "        performs mean normalization and other such operations'''\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dim(**kwargs):\n",
    "    ''' performs dimensionality reduction'''\n",
    "    if kwargs['method'] == 'lda':\n",
    "        lda=LDA(n_components=kwargs['no_of_components'])\n",
    "        lda = lda.fit(kwargs['train'],kwargs['y'])\n",
    "        train_new = lda.transform(kwargs['train'])\n",
    "        test_new = lda.transform(kwargs['test'])\n",
    "        return train_new, test_new\n",
    "    if kwargs['method'] == 'pca':\n",
    "        pca = PCA(n_components=kwargs['no_of_components'])\n",
    "        pca = pca.fit(kwargs['train'])\n",
    "        train_new = pca.transform(kwargs['train'])\n",
    "        test_new = pca.transform(kwargs['test'])\n",
    "        return train_new, test_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(X, y, **kwargs):\n",
    "    print('it came')\n",
    "    ''' trains a classifier by taking input features\n",
    "        and their respective targets and returns the trained model'''\n",
    "    if kwargs['method'] == 'LR':\n",
    "        lr = LogisticRegression(C=math.exp(kwargs['c']),solver='lbfgs',multi_class='multinomial',class_weight ='balanced',max_iter=kwargs['max_iterations'])\n",
    "        lr.fit(X, y)\n",
    "        print('it is classified')\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(target, predicted):\n",
    "    f1 = f1_score(target, predicted, average='micro')\n",
    "    acc = accuracy_score(target, predicted)\n",
    "    return f1, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(**kwargs):\n",
    "    '''takes test data and trained classifier model,\n",
    "    performs classification and prints accuracy and f1-score'''\n",
    "    if kwargs['method'] == 'LR':\n",
    "        lr = kwargs['model']\n",
    "        testdata = kwargs['testdata']\n",
    "        out = lr.predict(testdata)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    #print(trn_labels)\n",
    "    #pre_test,pre_train=preprocess(X_train,X_test)\n",
    "    pre_test,pre_train=X_train,X_test\n",
    "    i=25;\n",
    "    f1_array=[]\n",
    "    acc_array=[]\n",
    "    i_array=[]\n",
    "    while(i<500):\n",
    "    #train_new,test_new=reduce_dim(method='lda',train=pre_test,test=pre_train,y=y_train)\n",
    "        train_new,test_new=reduce_dim(method='pca',train=pre_test,test=pre_train,no_of_components=i)\n",
    "        #lr=classify(pre_test,trn_labels,method='LR')\n",
    "        lr=classify(train_new,y_train,method='LR',max_iterations=2000,c=5)\n",
    "        out= test(method='LR',model=lr,testdata=test_new)\n",
    "        f1,acc=evaluate(y_test,out)\n",
    "        f1_array.append(f1)\n",
    "        acc_array.append(acc)\n",
    "        i_array.append(i)\n",
    "        print('Val - F1 score: {}\\n Accuracy: {}'.format(f1, acc))\n",
    "        print(i)\n",
    "        i=i+25\n",
    "    \n",
    "    ''' perform dimesioality reduction/feature extraction and classify the features into one of 10 classses\n",
    "        print accuracy and f1-score.\n",
    "        '''\n",
    "    plt.plot(i_array,acc_array)\n",
    "    plt.xlabel('No. of pca components') \n",
    "    plt.ylabel('Accuracy') \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main2():\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    #print(trn_labels)\n",
    "    #pre_test,pre_train=preprocess(X_train,X_test)\n",
    "    pre_test,pre_train=X_train,X_test\n",
    "    \n",
    "    i=1000;\n",
    "    f1_array=[]\n",
    "    acc_array=[]\n",
    "    i_array=[]\n",
    "    while(i<3000):\n",
    "    #train_new,test_new=reduce_dim(method='lda',train=pre_test,test=pre_train,y=y_train)\n",
    "        train_new,test_new=reduce_dim(method='pca',train=pre_test,test=pre_train,no_of_components=150)\n",
    "        #lr=classify(pre_test,trn_labels,method='LR')\n",
    "        lr=classify(train_new,y_train,method='LR',max_iterations=i,c=5)\n",
    "        out= test(method='LR',model=lr,testdata=test_new)\n",
    "        f1,acc=evaluate(y_test,out)\n",
    "        f1_array.append(f1)\n",
    "        acc_array.append(acc)\n",
    "        i_array.append(i)\n",
    "        print('Val - F1 score: {}\\n Accuracy: {}'.format(f1, acc))\n",
    "        print(i)\n",
    "        i=i+200\n",
    "    plt.plot(i_array,acc_array)\n",
    "    plt.xlabel('Max number of iterations') \n",
    "    plt.ylabel('Accuracy') \n",
    "    plt.show()\n",
    "    i=1000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main3():\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    #print(trn_labels)\n",
    "    #pre_test,pre_train=preprocess(X_train,X_test)\n",
    "    pre_test,pre_train=X_train,X_test\n",
    "    \n",
    "    i=25;\n",
    "    f1_array=[]\n",
    "    acc_array=[]\n",
    "    i_array=[]\n",
    "    while(i<500):\n",
    "        train_new,test_new=reduce_dim(method='lda',train=pre_test,test=pre_train,y=y_train,no_of_components=i)\n",
    "        #train_new,test_new=reduce_dim(method='pca',train=pre_test,test=pre_train,no_of_components=150)\n",
    "        #train_new,test_new=pre_train,pre_test\n",
    "        #lr=classify(pre_test,trn_labels,method='LR')\n",
    "        lr=classify(train_new,y_train,method='LR',max_iterations=1000,c=5)\n",
    "        out= test(method='LR',model=lr,testdata=test_new)\n",
    "        f1,acc=evaluate(y_test,out)\n",
    "        f1_array.append(f1)\n",
    "        acc_array.append(acc)\n",
    "        i_array.append(i)\n",
    "        print('Val - F1 score: {}\\n Accuracy: {}'.format(f1, acc))\n",
    "        print(i)\n",
    "        i=i+50\n",
    "    plt.plot(i_array,acc_array)\n",
    "    plt.xlabel('NO of lda components') \n",
    "    plt.ylabel('Accuracy') \n",
    "    plt.show()\n",
    "    i=1000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main4():\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    #print(trn_labels)\n",
    "    #pre_test,pre_train=preprocess(X_train,X_test)\n",
    "    pre_test,pre_train=X_train,X_test\n",
    "    \n",
    "    i=1;\n",
    "    f1_array=[]\n",
    "    acc_array=[]\n",
    "    i_array=[]\n",
    "    while(i<7):\n",
    "    #train_new,test_new=reduce_dim(method='lda',train=pre_test,test=pre_train,y=y_train)\n",
    "        train_new,test_new=reduce_dim(method='pca',train=pre_test,test=pre_train,no_of_components=150)\n",
    "        #lr=classify(pre_test,trn_labels,method='LR')\n",
    "        lr=classify(train_new,y_train,method='LR',max_iterations=2400,c=i)\n",
    "        out= test(method='LR',model=lr,testdata=test_new)\n",
    "        f1,acc=evaluate(y_test,out)\n",
    "        f1_array.append(f1)\n",
    "        acc_array.append(acc)\n",
    "        i_array.append(i)\n",
    "        print('Val - F1 score: {}\\n Accuracy: {}'.format(f1, acc))\n",
    "        print(i)\n",
    "        i=i+1\n",
    "    plt.plot(i_array,acc_array)\n",
    "    plt.xlabel('regulirisation strength') \n",
    "    plt.ylabel('Accuracy') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
